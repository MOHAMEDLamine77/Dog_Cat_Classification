{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importation des donnees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "\n",
    "from keras.callbacks.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 21, 21, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 3, 64)          18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 23,617\n",
      "Trainable params: 23,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Conv2D(32, 3, 3, input_shape=(64, 64, 3), activation='relu'))\n",
    "\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(keras.layers.Conv2D(64, 3, 3, activation='relu'))\n",
    "\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(keras.layers.Flatten())\n",
    "\n",
    "model.add(keras.layers.Dense(64, activation='relu'))\n",
    "\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Epoch 1/20\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.6904 - acc: 0.5314Epoch 1/20\n",
      "300/300 [==============================] - 48s 161ms/step - loss: 0.6904 - acc: 0.5314 - val_loss: 0.6707 - val_acc: 0.6132\n",
      "Epoch 2/20\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.6661 - acc: 0.5957Epoch 1/20\n",
      "300/300 [==============================] - 47s 156ms/step - loss: 0.6660 - acc: 0.5961 - val_loss: 0.6696 - val_acc: 0.6100\n",
      "Epoch 3/20\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.6433 - acc: 0.6300Epoch 1/20\n",
      "300/300 [==============================] - 47s 156ms/step - loss: 0.6432 - acc: 0.6301 - val_loss: 0.6375 - val_acc: 0.6579\n",
      "Epoch 4/20\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.6229 - acc: 0.6514Epoch 1/20\n",
      "300/300 [==============================] - 48s 160ms/step - loss: 0.6228 - acc: 0.6516 - val_loss: 0.6646 - val_acc: 0.6068\n",
      "Epoch 5/20\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.6057 - acc: 0.6685Epoch 1/20\n",
      "300/300 [==============================] - 47s 155ms/step - loss: 0.6055 - acc: 0.6690 - val_loss: 0.6168 - val_acc: 0.6762\n",
      "Epoch 6/20\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.5909 - acc: 0.6823Epoch 1/20\n",
      "300/300 [==============================] - 47s 156ms/step - loss: 0.5911 - acc: 0.6821 - val_loss: 0.6431 - val_acc: 0.6435\n",
      "Epoch 7/20\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.5867 - acc: 0.6871Epoch 1/20\n",
      "300/300 [==============================] - 47s 158ms/step - loss: 0.5864 - acc: 0.6874 - val_loss: 0.6082 - val_acc: 0.6726\n",
      "Epoch 8/20\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.5847 - acc: 0.6916Epoch 1/20\n",
      "300/300 [==============================] - 48s 160ms/step - loss: 0.5852 - acc: 0.6910 - val_loss: 0.6039 - val_acc: 0.6931\n",
      "Epoch 9/20\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.5674 - acc: 0.7047Epoch 1/20\n",
      "300/300 [==============================] - 47s 157ms/step - loss: 0.5671 - acc: 0.7047 - val_loss: 0.6112 - val_acc: 0.6909\n",
      "Epoch 10/20\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.5664 - acc: 0.7068Epoch 1/20\n",
      "300/300 [==============================] - 47s 157ms/step - loss: 0.5667 - acc: 0.7068 - val_loss: 0.6002 - val_acc: 0.7024\n",
      "Epoch 11/20\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.5616 - acc: 0.7079Epoch 1/20\n",
      "300/300 [==============================] - 47s 157ms/step - loss: 0.5620 - acc: 0.7073 - val_loss: 0.5967 - val_acc: 0.6957\n",
      "Epoch 12/20\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.5551 - acc: 0.7172Epoch 1/20\n",
      "300/300 [==============================] - 45s 151ms/step - loss: 0.5549 - acc: 0.7175 - val_loss: 0.5922 - val_acc: 0.6870\n",
      "Epoch 13/20\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.5524 - acc: 0.7166Epoch 1/20\n",
      "300/300 [==============================] - 47s 158ms/step - loss: 0.5525 - acc: 0.7167 - val_loss: 0.6423 - val_acc: 0.6755\n",
      "Epoch 14/20\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.5460 - acc: 0.7183Epoch 1/20\n",
      "300/300 [==============================] - 48s 158ms/step - loss: 0.5463 - acc: 0.7179 - val_loss: 0.6377 - val_acc: 0.6752\n",
      "Epoch 15/20\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.5320 - acc: 0.7286Epoch 1/20\n",
      "300/300 [==============================] - 48s 158ms/step - loss: 0.5320 - acc: 0.7287 - val_loss: 0.5969 - val_acc: 0.6899\n",
      "Epoch 16/20\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.5321 - acc: 0.7298Epoch 1/20\n",
      "300/300 [==============================] - 48s 158ms/step - loss: 0.5319 - acc: 0.7299 - val_loss: 0.6078 - val_acc: 0.6921\n",
      "Epoch 17/20\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.5313 - acc: 0.7362Epoch 1/20\n",
      "300/300 [==============================] - 47s 157ms/step - loss: 0.5312 - acc: 0.7364 - val_loss: 0.6560 - val_acc: 0.6592\n",
      "Epoch 18/20\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.5183 - acc: 0.7452Epoch 1/20\n",
      "300/300 [==============================] - 48s 159ms/step - loss: 0.5182 - acc: 0.7453 - val_loss: 0.6392 - val_acc: 0.6544\n",
      "Epoch 19/20\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.5225 - acc: 0.7386Epoch 1/20\n",
      "300/300 [==============================] - 47s 156ms/step - loss: 0.5224 - acc: 0.7389 - val_loss: 0.5926 - val_acc: 0.6918\n",
      "Epoch 20/20\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.5124 - acc: 0.7473Epoch 1/20\n",
      "300/300 [==============================] - 47s 157ms/step - loss: 0.5123 - acc: 0.7475 - val_loss: 0.6174 - val_acc: 0.6909\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1bad7ecd940>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "                                    rescale=1./255,\n",
    "                                    shear_range=0.2,\n",
    "                                    zoom_range=0.2,\n",
    "                                    horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "                                                'D:\\\\Deep Learning\\\\DATAsets\\\\Data for CNN Classification\\\\mini_dataset_dog and cat\\\\training_set',\n",
    "                                                target_size=(64, 64),\n",
    "                                                batch_size=32,\n",
    "                                                class_mode='binary')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "                                            'D:\\\\Deep Learning\\\\DATAsets\\\\Data for CNN Classification\\\\mini_dataset_dog and cat\\\\test_set',\n",
    "                                            target_size=(64, 64),\n",
    "                                            batch_size=32,\n",
    "                                            class_mode='binary')\n",
    "\n",
    "model.fit_generator(\n",
    "                    training_set,\n",
    "                    steps_per_epoch=300,\n",
    "                    epochs=20,\n",
    "                    validation_data=test_set,\n",
    "                    validation_steps=100) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation du model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 17s 133ms/step - loss: 0.4946 - acc: 0.7605\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = model.evaluate(training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Charger l'image pour la prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('D:\\\\Deep Learning\\\\DATAsets\\\\Data for CNN Classification\\\\mini_dataset_dog and cat\\\\modelcnnsauv.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = image.load_img('D:\\\\Deep Learning\\\\DATAsets\\\\Data for CNN Classification\\\\mini_dataset_dog and cat\\\\pour predire\\\\cat.3357.jpg', target_size=(64, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion de l'image en matrice pour pouvoir la redimensionner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = image.img_to_array(test_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redimensionner l'image pour quelle soit compatible avec notre couche de convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = np.expand_dims(test_img, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faire une prediction sur la nouvelle image esque elle appartien a la classe 1 'chien' ou la classe 0 'chat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "verifier les valeurs des classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat': 0, 'chien': 1}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Condition pour afficher directement la prediction comme nom et non pas comme valeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n"
     ]
    }
   ],
   "source": [
    "if result[0][0] == 1:\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'\n",
    "    \n",
    "    \n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "et maintenant on va predire pour une 2eme image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]]\n"
     ]
    }
   ],
   "source": [
    "test_img2 = image.load_img('D:\\\\Deep Learning\\\\DATAsets\\\\Data for CNN Classification\\\\mini_dataset_dog and cat\\\\pour predire\\\\dog.1951.jpg', target_size=(64, 64))\n",
    "\n",
    "test_img2 = image.img_to_array(test_img2)\n",
    "\n",
    "test_img2 = np.expand_dims(test_img2, axis=0)\n",
    "\n",
    "result2 = model.predict(test_img2)\n",
    "\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n"
     ]
    }
   ],
   "source": [
    "if result2[0][0] == 1:\n",
    "    prediction2 = 'dog'\n",
    "    \n",
    "else:\n",
    "    prediction2 = 'cat'\n",
    "    \n",
    "print(prediction2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
