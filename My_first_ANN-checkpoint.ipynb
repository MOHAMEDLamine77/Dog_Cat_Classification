{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the librairy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('D:\\\\Deep Learning\\\\DATAsets\\\\Data for DNN\\\\Churn_Modelling.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take important values in the data set for features and take a target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, 3:13]\n",
    "Y = data.iloc[:, 13]\n",
    "\n",
    "X_1 = X.to_numpy()\n",
    "Y_1 = Y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 10)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_1.shape)\n",
    "print(Y_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[619 'France' 'Female' ... 1 1 101348.88]\n",
      " [608 'Spain' 'Female' ... 0 1 112542.58]\n",
      " [502 'France' 'Female' ... 1 0 113931.57]\n",
      " ...\n",
      " [709 'France' 'Female' ... 0 1 42085.58]\n",
      " [772 'Germany' 'Male' ... 1 0 92888.52]\n",
      " [792 'France' 'Female' ... 1 0 38190.78]]\n"
     ]
    }
   ],
   "source": [
    "print(X_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 ... 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(Y_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we must encode the string values to the digit values because our network calculate the digits not the words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_X_1 = LabelEncoder()\n",
    "X_1[:, 1] = lab_X_1.fit_transform(X_1[:, 1])\n",
    "\n",
    "lab_X_2 = LabelEncoder()\n",
    "X_1[:, 2] = lab_X_2.fit_transform(X_1[:, 2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing the result of encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[619 0 0 ... 1 1 101348.88]\n",
      " [608 2 0 ... 0 1 112542.58]\n",
      " [502 0 0 ... 1 0 113931.57]\n",
      " ...\n",
      " [709 0 0 ... 0 1 42085.58]\n",
      " [772 1 1 ... 1 0 92888.52]\n",
      " [792 0 0 ... 1 0 38190.78]]\n"
     ]
    }
   ],
   "source": [
    "print(X_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is ambiguest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = ColumnTransformer([(\"Country\", OneHotEncoder(), [1])], remainder = 'passthrough')\n",
    "\n",
    "X_1 = ct.fit_transform(X_1)\n",
    "\n",
    "X_1 = X_1[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 0.0 619 ... 1 1 101348.88]\n",
      " [0.0 1.0 608 ... 0 1 112542.58]\n",
      " [0.0 0.0 502 ... 1 0 113931.57]\n",
      " ...\n",
      " [0.0 0.0 709 ... 0 1 42085.58]\n",
      " [1.0 0.0 772 ... 1 0 92888.52]\n",
      " [0.0 0.0 792 ... 1 0 38190.78]]\n"
     ]
    }
   ],
   "source": [
    "print(X_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0 0.0 699 0 39 1 0.0 2 0 0 93826.63]\n"
     ]
    }
   ],
   "source": [
    "print(X_1[3, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X_1, Y_1, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data scaling ( Normalization )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.fit_transform(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 11)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dense(11, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dropout(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dense(11, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dropout(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "Train on 8000 samples\n",
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 1s 162us/sample - loss: 0.5322 - accuracy: 0.7940\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 1s 79us/sample - loss: 0.4807 - accuracy: 0.7960\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 1s 77us/sample - loss: 0.4579 - accuracy: 0.7960\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 1s 78us/sample - loss: 0.4443 - accuracy: 0.7959\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 1s 74us/sample - loss: 0.4388 - accuracy: 0.7956\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 1s 77us/sample - loss: 0.4315 - accuracy: 0.7960\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 1s 77us/sample - loss: 0.4227 - accuracy: 0.7966\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 1s 77us/sample - loss: 0.4181 - accuracy: 0.8012\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 1s 77us/sample - loss: 0.4040 - accuracy: 0.8231\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 1s 77us/sample - loss: 0.3943 - accuracy: 0.8371\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 1s 79us/sample - loss: 0.3823 - accuracy: 0.8404\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 1s 76us/sample - loss: 0.3819 - accuracy: 0.8444\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 1s 81us/sample - loss: 0.3775 - accuracy: 0.8451\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 1s 88us/sample - loss: 0.3756 - accuracy: 0.8462\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 1s 81us/sample - loss: 0.3734 - accuracy: 0.8493\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 1s 84us/sample - loss: 0.3725 - accuracy: 0.8474\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 1s 77us/sample - loss: 0.3718 - accuracy: 0.8472\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 1s 79us/sample - loss: 0.3700 - accuracy: 0.8475\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 1s 73us/sample - loss: 0.3736 - accuracy: 0.8451\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 1s 80us/sample - loss: 0.3663 - accuracy: 0.8494\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 1s 78us/sample - loss: 0.3675 - accuracy: 0.8503\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 1s 79us/sample - loss: 0.3686 - accuracy: 0.8505\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 1s 77us/sample - loss: 0.3684 - accuracy: 0.8461\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 1s 78us/sample - loss: 0.3672 - accuracy: 0.8504\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 1s 67us/sample - loss: 0.3661 - accuracy: 0.8493\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 1s 86us/sample - loss: 0.3680 - accuracy: 0.8499\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 1s 76us/sample - loss: 0.3689 - accuracy: 0.8484\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 1s 77us/sample - loss: 0.3674 - accuracy: 0.8499\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 1s 77us/sample - loss: 0.3645 - accuracy: 0.8516\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 1s 76us/sample - loss: 0.3633 - accuracy: 0.8500\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 1s 83us/sample - loss: 0.3660 - accuracy: 0.8518\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 1s 80us/sample - loss: 0.3669 - accuracy: 0.8504\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 1s 79us/sample - loss: 0.3661 - accuracy: 0.8500\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 1s 83us/sample - loss: 0.3651 - accuracy: 0.8526\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 1s 78us/sample - loss: 0.3622 - accuracy: 0.8521\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 1s 77us/sample - loss: 0.3634 - accuracy: 0.8494\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 1s 80us/sample - loss: 0.3608 - accuracy: 0.8528\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 1s 80us/sample - loss: 0.3629 - accuracy: 0.8543\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 1s 82us/sample - loss: 0.3622 - accuracy: 0.8540\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 1s 80us/sample - loss: 0.3646 - accuracy: 0.8543\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 1s 79us/sample - loss: 0.3669 - accuracy: 0.8509\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 1s 79us/sample - loss: 0.3614 - accuracy: 0.8509\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 1s 72us/sample - loss: 0.3594 - accuracy: 0.8545\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 1s 74us/sample - loss: 0.3595 - accuracy: 0.8535\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 1s 84us/sample - loss: 0.3623 - accuracy: 0.8525\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 1s 82us/sample - loss: 0.3620 - accuracy: 0.8526\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 1s 72us/sample - loss: 0.3625 - accuracy: 0.8539s - loss: 0.3\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 1s 78us/sample - loss: 0.3583 - accuracy: 0.8522\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 1s 83us/sample - loss: 0.3585 - accuracy: 0.8547\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 1s 80us/sample - loss: 0.3562 - accuracy: 0.8569\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 1s 80us/sample - loss: 0.3609 - accuracy: 0.8497\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 1s 80us/sample - loss: 0.3583 - accuracy: 0.8539\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 1s 86us/sample - loss: 0.3589 - accuracy: 0.8535\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 1s 79us/sample - loss: 0.3593 - accuracy: 0.8533\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 1s 79us/sample - loss: 0.3556 - accuracy: 0.8550\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 1s 79us/sample - loss: 0.3573 - accuracy: 0.8529\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 1s 80us/sample - loss: 0.3569 - accuracy: 0.8536\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 1s 81us/sample - loss: 0.3606 - accuracy: 0.8571\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 1s 86us/sample - loss: 0.3567 - accuracy: 0.8570\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 1s 80us/sample - loss: 0.3567 - accuracy: 0.8558\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 1s 81us/sample - loss: 0.3593 - accuracy: 0.8560\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 1s 85us/sample - loss: 0.3579 - accuracy: 0.8516\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 1s 81us/sample - loss: 0.3582 - accuracy: 0.8558\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 1s 79us/sample - loss: 0.3601 - accuracy: 0.8530\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 1s 79us/sample - loss: 0.3605 - accuracy: 0.8521\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 1s 79us/sample - loss: 0.3615 - accuracy: 0.8537\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 1s 80us/sample - loss: 0.3594 - accuracy: 0.8531\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 0s 59us/sample - loss: 0.3549 - accuracy: 0.8566\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 1s 67us/sample - loss: 0.3571 - accuracy: 0.8534\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 1s 76us/sample - loss: 0.3551 - accuracy: 0.8512\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 1s 68us/sample - loss: 0.3581 - accuracy: 0.8587\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 1s 75us/sample - loss: 0.3556 - accuracy: 0.8541\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 1s 80us/sample - loss: 0.3561 - accuracy: 0.8515\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 1s 85us/sample - loss: 0.3529 - accuracy: 0.8572\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 1s 83us/sample - loss: 0.3579 - accuracy: 0.8546\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 1s 82us/sample - loss: 0.3567 - accuracy: 0.8541\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 1s 82us/sample - loss: 0.3538 - accuracy: 0.8586\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 1s 81us/sample - loss: 0.3557 - accuracy: 0.8546\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 1s 82us/sample - loss: 0.3546 - accuracy: 0.8531\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 1s 83us/sample - loss: 0.3534 - accuracy: 0.8555\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 1s 86us/sample - loss: 0.3545 - accuracy: 0.8544\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 1s 82us/sample - loss: 0.3532 - accuracy: 0.8555\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 1s 79us/sample - loss: 0.3507 - accuracy: 0.8591\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 1s 76us/sample - loss: 0.3526 - accuracy: 0.8558\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 1s 81us/sample - loss: 0.3524 - accuracy: 0.8543\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 1s 80us/sample - loss: 0.3523 - accuracy: 0.8566\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 1s 80us/sample - loss: 0.3553 - accuracy: 0.8546\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 1s 82us/sample - loss: 0.3532 - accuracy: 0.8555\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 1s 81us/sample - loss: 0.3538 - accuracy: 0.8533s - loss: 0\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 1s 76us/sample - loss: 0.3561 - accuracy: 0.8537\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 1s 86us/sample - loss: 0.3533 - accuracy: 0.8553\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 1s 75us/sample - loss: 0.3519 - accuracy: 0.8544\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 1s 71us/sample - loss: 0.3534 - accuracy: 0.8539\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 1s 74us/sample - loss: 0.3487 - accuracy: 0.8560\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 1s 65us/sample - loss: 0.3497 - accuracy: 0.8570s - loss: 0.3442 \n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 1s 81us/sample - loss: 0.3520 - accuracy: 0.8543\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 1s 84us/sample - loss: 0.3497 - accuracy: 0.8561\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 1s 81us/sample - loss: 0.3491 - accuracy: 0.8564\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 1s 74us/sample - loss: 0.3555 - accuracy: 0.8543\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 1s 76us/sample - loss: 0.3514 - accuracy: 0.8558\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b0df58f9b0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, nb_epoch=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 84us/sample - loss: 0.3429 - accuracy: 0.8615\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predic = model.predict(x_test)\n",
    "y_predic = (y_predic > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing two random customer boolean prediction 'True' mean leave the bank 'False' mean not leave the bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True]\n",
      "=====================================\n",
      "[False]\n"
     ]
    }
   ],
   "source": [
    "print(y_predic[602])\n",
    "print('=====================================')\n",
    "print(y_predic[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "confu = confusion_matrix(y_test, y_predic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 1513 + 196 of correct prediction and 209 + 82 of false prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1502   93]\n",
      " [ 184  221]]\n"
     ]
    }
   ],
   "source": [
    "print(confu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.5698444   1.74309049  0.16958176 -1.09168714 -0.46460796  0.00666099\n",
      " -1.21571749  0.8095029   0.64259497 -1.03227043  1.10643166]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pridicting if an customer will leave the bank or not the information of the customer are so below:\n",
    "    - Geography: France\n",
    "    - Credit Score: 600\n",
    "    - Gender: Male\n",
    "    - Age: 40 years old\n",
    "    - Tenure: 3 years\n",
    "    - Balance: 60000\n",
    "    - Number of Products: 2\n",
    "    - Does this customer have a credit card ? Yes\n",
    "    - Is this customer an Active Member: Yes\n",
    "    - Estimated Salary: 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc1 = StandardScaler()\n",
    "new_predic = model.predict(sc1.fit_transform(np.array([[0.0, 0, 600, 1, 40, 3, 60000, 2, 1, 1, 50000]])))\n",
    "new_predic = (new_predic > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False]]\n"
     ]
    }
   ],
   "source": [
    "print(new_predic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will make the cross k_folds validation method for showing the real accuracy of our model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In first we import the librairy who's make the link between keras and scikitlearn because our model is implementing in keras and the cross vladition method is in the scikit learn librairy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we import the scikitlearn librairy for cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will remake our model but inside a function 'don't execute the model above'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Build_classifier():\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Dense(11, activation='relu'))\n",
    "    model.add(keras.layers.Dense(11, activation='relu'))\n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start execution of k-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=Build_classifier, batch_size=10, nb_epoch=100)\n",
    "accuracies = cross_val_score(estimator=model, X=x_train, y=y_train, cv=10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing a vector of accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.79374999 0.81875002 0.8075     0.79374999 0.83999997 0.80500001\n",
      " 0.79374999 0.80124998 0.8125     0.83875   ]\n"
     ]
    }
   ],
   "source": [
    "print(accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The real accuracy of our model is the average of vector of accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la veritable precision de notre model est:  0.8104999959468842\n"
     ]
    }
   ],
   "source": [
    "moy = accuracies.mean()\n",
    "print('la veritable precision de notre model est: ', moy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also verify our variance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la variance de notre model est:  0.016452585082878456\n"
     ]
    }
   ],
   "source": [
    "vari = accuracies.std()\n",
    "print('la variance de notre model est: ', vari)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for ameliorate our precision we apply the gridsearch method for searching the best hyper parameteres for our model and this method use the model and the cross validation in the same time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first we import the gridsearch method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement the model inside the function 'the same function of cross validation' but here we change the hyper parameteres "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Build_classifier(optimizer):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Dense(11, activation='relu'))\n",
    "    model.add(keras.layers.Dense(11, activation='relu'))\n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=Build_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will defind our hyper parameteres to will be change after searching the best one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'batch_size': [25, 32],\n",
    "         'nb_epoch': [100, 300],\n",
    "         'optimizer':['adam', 'rmsprop']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the gridsearch object for starting searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 155us/sample - loss: 0.5174 - accuracy: 0.7807\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 154us/sample - loss: 0.4715 - accuracy: 0.7942\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 160us/sample - loss: 0.5414 - accuracy: 0.7553\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 159us/sample - loss: 0.5815 - accuracy: 0.7224\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 155us/sample - loss: 0.4937 - accuracy: 0.7844\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 159us/sample - loss: 0.4826 - accuracy: 0.7956\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 160us/sample - loss: 0.4821 - accuracy: 0.7969\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 154us/sample - loss: 0.5021 - accuracy: 0.7843\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 146us/sample - loss: 0.4714 - accuracy: 0.7994\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 153us/sample - loss: 0.5223 - accuracy: 0.7786\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 164us/sample - loss: 0.5455 - accuracy: 0.7588\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 167us/sample - loss: 0.5243 - accuracy: 0.7754\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 166us/sample - loss: 0.4916 - accuracy: 0.7953\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 167us/sample - loss: 0.5222 - accuracy: 0.7663\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 166us/sample - loss: 0.5355 - accuracy: 0.7628\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 171us/sample - loss: 0.5248 - accuracy: 0.7936\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 175us/sample - loss: 0.5209 - accuracy: 0.7614\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 204us/sample - loss: 0.4823 - accuracy: 0.7947\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 178us/sample - loss: 0.5466 - accuracy: 0.7525\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 153us/sample - loss: 0.5536 - accuracy: 0.7340\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 152us/sample - loss: 0.5376 - accuracy: 0.7493\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 149us/sample - loss: 0.5682 - accuracy: 0.7322\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 162us/sample - loss: 0.5377 - accuracy: 0.7686\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 164us/sample - loss: 0.5347 - accuracy: 0.7396\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 157us/sample - loss: 0.5291 - accuracy: 0.7835\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 165us/sample - loss: 0.5437 - accuracy: 0.7640\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 159us/sample - loss: 0.5837 - accuracy: 0.6836\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 158us/sample - loss: 0.5006 - accuracy: 0.7961\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 166us/sample - loss: 0.5355 - accuracy: 0.7287\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 154us/sample - loss: 0.5158 - accuracy: 0.7683\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 175us/sample - loss: 0.5659 - accuracy: 0.7118\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 166us/sample - loss: 0.4700 - accuracy: 0.7986\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 172us/sample - loss: 0.5197 - accuracy: 0.7794\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 167us/sample - loss: 0.5505 - accuracy: 0.7333\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 164us/sample - loss: 0.5100 - accuracy: 0.7697\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 167us/sample - loss: 0.4795 - accuracy: 0.7947\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 180us/sample - loss: 0.4961 - accuracy: 0.7818\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 205us/sample - loss: 0.5642 - accuracy: 0.7333\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 179us/sample - loss: 0.5337 - accuracy: 0.7585\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 173us/sample - loss: 0.4943 - accuracy: 0.7932\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 127us/sample - loss: 0.5827 - accuracy: 0.7038\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 134us/sample - loss: 0.6731 - accuracy: 0.6124\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 141us/sample - loss: 0.5293 - accuracy: 0.7647\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 135us/sample - loss: 0.5124 - accuracy: 0.7781\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 126us/sample - loss: 0.6809 - accuracy: 0.6325\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 132us/sample - loss: 0.5196 - accuracy: 0.7849\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 135us/sample - loss: 0.6237 - accuracy: 0.6636\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 137us/sample - loss: 0.5813 - accuracy: 0.7118\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 172us/sample - loss: 0.6298 - accuracy: 0.6460\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 134us/sample - loss: 0.5259 - accuracy: 0.7474\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 146us/sample - loss: 0.5369 - accuracy: 0.7706\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 162us/sample - loss: 0.5421 - accuracy: 0.7833\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 151us/sample - loss: 0.5071 - accuracy: 0.7860\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - ETA: 0s - loss: 0.5506 - accuracy: 0.77 - 1s 144us/sample - loss: 0.5469 - accuracy: 0.7744\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 148us/sample - loss: 0.5297 - accuracy: 0.7886\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 143us/sample - loss: 0.5774 - accuracy: 0.7467\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 144us/sample - loss: 0.5317 - accuracy: 0.7792\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 130us/sample - loss: 0.5310 - accuracy: 0.7728\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 147us/sample - loss: 0.4920 - accuracy: 0.7871\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 143us/sample - loss: 0.5610 - accuracy: 0.7383\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 132us/sample - loss: 0.5345 - accuracy: 0.7690\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 129us/sample - loss: 0.5293 - accuracy: 0.7729\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 123us/sample - loss: 0.5478 - accuracy: 0.7801\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 125us/sample - loss: 0.5277 - accuracy: 0.7656\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 142us/sample - loss: 0.5129 - accuracy: 0.7893\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 131us/sample - loss: 0.4956 - accuracy: 0.7915\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 139us/sample - loss: 0.6752 - accuracy: 0.6118\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 140us/sample - loss: 0.5842 - accuracy: 0.7189\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 133us/sample - loss: 0.5191 - accuracy: 0.7814\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 144us/sample - loss: 0.5162 - accuracy: 0.7818\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 155us/sample - loss: 0.5422 - accuracy: 0.7586\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 167us/sample - loss: 0.5448 - accuracy: 0.7575\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 149us/sample - loss: 0.4876 - accuracy: 0.7947\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 166us/sample - loss: 0.6133 - accuracy: 0.6753\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 159us/sample - loss: 0.5378 - accuracy: 0.7675\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 157us/sample - loss: 0.5199 - accuracy: 0.7693\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 164us/sample - loss: 0.5428 - accuracy: 0.7622\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 169us/sample - loss: 0.5393 - accuracy: 0.7881\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 155us/sample - loss: 0.5789 - accuracy: 0.7161\n",
      "Train on 7200 samples\n",
      "7200/7200 [==============================] - 1s 137us/sample - loss: 0.5317 - accuracy: 0.7643\n",
      "Train on 8000 samples\n",
      "8000/8000 [==============================] - 1s 147us/sample - loss: 0.4924 - accuracy: 0.7891\n"
     ]
    }
   ],
   "source": [
    "grid_search = GridSearchCV(estimator=model, param_grid=param, scoring='accuracy', cv=10)\n",
    "grid_search = grid_search.fit(x_train, y_train)\n",
    "best_parameters = grid_search.best_params_\n",
    "best_accuracy = grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
